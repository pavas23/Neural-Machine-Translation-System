{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¤ Une coalition (BBY) composée de plus d’une c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$Mais le groupe a aussi annoncé la suppression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$Toutefois, l’étude menée a également livré de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>€Dans les locaux d’Atem, à Solliès-Pont, les é...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>° 1.000.000 de dirhams hors taxes pour les mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>С’est son premier voyage à l’étranger depuis s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>С’est un établissement d'enseignement supérieu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>Сергей ГунеевAccéder à la base multimédiaVladi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>يا تونِسَ الأُنسِ يا خَضرا المَيادينِ », a écr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>チーズ, c’est le mot japonais pour “fromage”.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "0       ¤ Une coalition (BBY) composée de plus d’une c...\n",
       "1       $Mais le groupe a aussi annoncé la suppression...\n",
       "2       $Toutefois, l’étude menée a également livré de...\n",
       "3       €Dans les locaux d’Atem, à Solliès-Pont, les é...\n",
       "4       ° 1.000.000 de dirhams hors taxes pour les mar...\n",
       "...                                                   ...\n",
       "999995  С’est son premier voyage à l’étranger depuis s...\n",
       "999996  С’est un établissement d'enseignement supérieu...\n",
       "999997  Сергей ГунеевAccéder à la base multimédiaVladi...\n",
       "999998  يا تونِسَ الأُنسِ يا خَضرا المَيادينِ », a écr...\n",
       "999999       チーズ, c’est le mot japonais pour “fromage”.\\n\n",
       "\n",
       "[1000000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load French Monolingual Dataset\n",
    "sentences = []\n",
    "\n",
    "with open('./Dataset/Monolingual/fra_news_2023_1M/fra_news_2023_1M-sentences.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split('\\t', 1)\n",
    "        if len(parts) > 1:\n",
    "            sentence = parts[1]\n",
    "            sentences.append(sentence)\n",
    "\n",
    "french_df = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "french_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$10,000 Gold?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAN FRANCISCO – It has never been easy to have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lately, with gold prices up more than 300% ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just last December, fellow economists Martin F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wouldn’t you know it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901593</th>\n",
       "      <td>At the same time, Zuma’s revolutionary generat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901594</th>\n",
       "      <td>In a region that reveres the elderly, Zuma’s a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901595</th>\n",
       "      <td>Three in ten South Africans are younger than 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901596</th>\n",
       "      <td>Somehow Zuma must find a way to honor his own ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901597</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>901598 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "0                                           $10,000 Gold?\n",
       "1       SAN FRANCISCO – It has never been easy to have...\n",
       "2       Lately, with gold prices up more than 300% ove...\n",
       "3       Just last December, fellow economists Martin F...\n",
       "4                                   Wouldn’t you know it?\n",
       "...                                                   ...\n",
       "901593  At the same time, Zuma’s revolutionary generat...\n",
       "901594  In a region that reveres the elderly, Zuma’s a...\n",
       "901595  Three in ten South Africans are younger than 1...\n",
       "901596  Somehow Zuma must find a way to honor his own ...\n",
       "901597                                                   \n",
       "\n",
       "[901598 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English Monolingual Dataset\n",
    "with open('./Dataset/Monolingual/news-commentary-v18.txt', 'r', encoding='utf-8') as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "english_df = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "english_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 20\n",
    "french_df = french_df[:x]\n",
    "english_df = english_df[:x]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty sentences if any\n",
    "english_df = english_df[english_df['Sentence'].notna() & (english_df['Sentence'] != '')]\n",
    "french_df = french_df[french_df['Sentence'].notna() & (french_df['Sentence'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)  # Remove punctuation\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.loc[:, 'Sentence'] = english_df['Sentence'].apply(lambda x: preprocess_text(x))\n",
    "french_df.loc[:, 'Sentence'] = french_df['Sentence'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remerge_sent(sent):\n",
    "    # merges tokens which are not separated by white-space\n",
    "    # does this recursively until no further changes\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        i = 0\n",
    "        while i < sent.__len__() - 1:\n",
    "            tok = sent[i]\n",
    "            if not tok.whitespace_:\n",
    "                ntok = sent[i + 1]\n",
    "                # in-place operation.\n",
    "                with sent.retokenize() as retokenizer:\n",
    "                    retokenizer.merge(sent[i: i + 2])\n",
    "                changed = True\n",
    "            i += 1\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_spacy(sentence, nlp):\n",
    "    doc = nlp(sentence)\n",
    "    spacy_sentence = remerge_sent(doc)\n",
    "    return [token.text for token in spacy_sentence]\n",
    "\n",
    "# Tokenize\n",
    "english_df['Tokens'] = english_df['Sentence'].apply(lambda x: tokenize_with_spacy(x, nlp_en))\n",
    "french_df['Tokens'] = french_df['Sentence'].apply(lambda x: tokenize_with_spacy(x, nlp_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop sentences longer than 50 words\n",
    "english_df = english_df[english_df['Tokens'].apply(len) <= 50]\n",
    "french_df = french_df[french_df['Tokens'].apply(len) <= 50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate BERT Embeddings\n",
    "\n",
    "We can use pre trained transformer models for generating embeddings for the tokens, we can use different versions of BERT for generation embeddings for French, German etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT tokenizer and model\n",
    "# Load the fast version of the tokenizer\n",
    "\n",
    "tokenizer_en = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "bert_model_en = AutoModel.from_pretrained('bert-base-uncased')\n",
    "bert_model_en = bert_model_en.to(device)\n",
    "\n",
    "tokenizer_fr = AutoTokenizer.from_pretrained('camembert-base', use_fast=True)\n",
    "bert_model_fr = AutoModel.from_pretrained('camembert-base')\n",
    "bert_model_fr = bert_model_fr.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves BERT embeddings for a list of tokens using a fast tokenizer, enabling accurate aggregation of subword embeddings into their original token representations.\n",
    "def get_bert_embeddings(tokens, tokenizer, bert_model):\n",
    "    inputs = tokenizer(tokens, return_tensors='pt', is_split_into_words=True, padding=False, truncation=True)\n",
    "\n",
    "    # Get BERT embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Get the embeddings for each subword\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: (sequence_length, hidden_size)\n",
    "    # Get word_ids to align subword tokens with the original tokens\n",
    "    word_ids = inputs.word_ids()\n",
    "\n",
    "    # Aggregate subword embeddings back to their original tokens\n",
    "    aggregated_embeddings = []\n",
    "    current_token_embeddings = []\n",
    "\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        if len(current_token_embeddings) > 0 and word_id != word_ids[idx - 1]:\n",
    "            aggregated_embeddings.append(torch.mean(torch.stack(current_token_embeddings), dim=0))\n",
    "            current_token_embeddings = []\n",
    "        current_token_embeddings.append(token_embeddings[idx])\n",
    "    \n",
    "    if len(current_token_embeddings) > 0:\n",
    "        aggregated_embeddings.append(torch.mean(torch.stack(current_token_embeddings), dim=0))\n",
    "\n",
    "    return torch.stack(aggregated_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate BERT embeddings for dataFrame\n",
    "def generate_embeddings(df, tokenizer, bert_model):\n",
    "    embeddings_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        tokenized_sentence = row['Tokens']\n",
    "        embeddings = get_bert_embeddings(tokenized_sentence, tokenizer, bert_model)\n",
    "        embeddings_list.append(embeddings)\n",
    "    return embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_embeddings(df, embedding_list):\n",
    "    # Checking the length of longest sentence\n",
    "    max_len = max(embedding.shape[0] for embedding in embedding_list)\n",
    "    print(f\"The length of the longest sentence is: {max_len}\")\n",
    "\n",
    "    # Pad the embeddings to ensure uniformity across dataset as model will be trained in batches\n",
    "    padded_embeddings = pad_sequence(embedding_list, batch_first=True)\n",
    "    df['Embeddings'] = [padded_embeddings[i] for i in range(padded_embeddings.shape[0])]\n",
    "    print(padded_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_embeddings = generate_embeddings(english_df, tokenizer_en, bert_model_en)\n",
    "english_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest sentence is: 31\n",
      "torch.Size([20, 31, 768])\n"
     ]
    }
   ],
   "source": [
    "generate_padded_embeddings(english_df, english_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_embeddings = generate_embeddings(french_df, tokenizer_fr, bert_model_fr)\n",
    "french_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest sentence is: 35\n",
      "torch.Size([20, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "generate_padded_embeddings(french_df, french_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Auto Encoding (DAE)\n",
    "\n",
    "Noise is added to make sure that self auto encoding mechanism is not just returning the same sequence of words, and rather learns the structure of the sentence, otherwise for a random sequence of words also it will return the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_dropout(sentence, pwd=0.1):\n",
    "    noisy_sentence = []\n",
    "\n",
    "    for word in sentence:\n",
    "        if random.random() > pwd:\n",
    "            # Drop the word with probability pwd\n",
    "            noisy_sentence.append(word)\n",
    "    return noisy_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentence_shuffling(sentence, k=3, alpha=0.5):\n",
    "    n = len(sentence)\n",
    "\n",
    "    # Generating random permutation vector q\n",
    "    q = np.array([i + random.uniform(0, alpha) for i in range(n)])\n",
    "    \n",
    "    # Sort the indices based on q, which gives the new positions for the words\n",
    "    permuted_indices = np.argsort(q)\n",
    "    \n",
    "    shuffled_sentence = [sentence[i] for i in permuted_indices]\n",
    "    \n",
    "    # Ensure the distance condition |σ(i) - i| <= k\n",
    "    for i in range(n):\n",
    "        if abs(permuted_indices[i] - i) > k:\n",
    "            # Revert to original if the condition fails\n",
    "            shuffled_sentence[i] = sentence[i]  \n",
    "    \n",
    "    return shuffled_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising_autoencoder(x, pwd=0.1, k=3, alpha=0.5):\n",
    "    # Apply word dropout\n",
    "    x_noisy = apply_word_dropout(x, pwd)\n",
    "    \n",
    "    # Apply sentence shuffling\n",
    "    x_noisy = apply_sentence_shuffling(x_noisy, k, alpha)\n",
    "    \n",
    "    return x_noisy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM Encoder\n",
    "\n",
    "Generates sequence of hidden (latent) states for a given sequence of input embeddings (src/tgt language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True, num_layers=3)\n",
    "        \n",
    "        # Fully connected layers for final prediction\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()  # Activation function\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Apply dropout and batch normalization\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = self.batch_norm(lstm_out.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        fc_output = self.activation(self.fc1(lstm_out))  # (batch_size, seq_len, hidden_dim)\n",
    "        fc_output = self.activation(self.fc2(fc_output))  # (batch_size, seq_len, hidden_dim // 2)\n",
    "        latent_output = self.fc3(fc_output)  # (batch_size, seq_len, output_dim)\n",
    "        \n",
    "        return latent_output\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 768  # BERT Embedding size\n",
    "hidden_dim = 300\n",
    "output_dim = 100  # Size of the latent state\n",
    "\n",
    "# Initialize model\n",
    "encoder_model = Encoder(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Decoder\n",
    "\n",
    "Generates sequence of words (src/tgt) language based on the previous hidden state, the current word, and a context vector given by a weighted sum over the encoder states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, latent_dim, embed_dim):\n",
    "#         super(Attention, self).__init__()\n",
    "#         # Define the attention layer\n",
    "#         self.attn = nn.Linear(latent_dim + embed_dim, embed_dim)\n",
    "#         self.v = nn.Parameter(torch.rand(embed_dim))\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         \"\"\"\n",
    "#         Compute the attention weights and the context vector.\n",
    "        \n",
    "#         Args:\n",
    "#             hidden: Previous hidden state of the decoder (batch_size, embed_dim)\n",
    "#             encoder_outputs: All hidden states from the encoder (batch_size, seq_len, latent_dim)\n",
    "        \n",
    "#         Returns:\n",
    "#             attn_weights: Attention weights (batch_size, seq_len)\n",
    "#         \"\"\"\n",
    "#         batch_size = encoder_outputs.size(0)\n",
    "#         max_len = encoder_outputs.size(1)\n",
    "\n",
    "#         # Expand hidden to (batch_size, seq_len, latent_dim) for concatenation\n",
    "#         hidden = hidden.unsqueeze(1).expand(batch_size, max_len, -1)\n",
    "\n",
    "#         # Concatenate hidden state and encoder outputs\n",
    "#         energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "#         # Compute attention weights\n",
    "#         energy = energy.transpose(1, 2)  # (batch_size, embed_dim, seq_len)\n",
    "#         v = self.v.repeat(batch_size, 1).unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "#         attn_weights = torch.bmm(v, energy).squeeze(1)  # (batch_size, seq_len)\n",
    "#         attn_weights = F.softmax(attn_weights, dim=1)  # Normalize over seq_len\n",
    "        \n",
    "#         return attn_weights\n",
    "\n",
    "# class AttentionCombine(nn.Module):\n",
    "#     def __init__(self, latent_dim, embed_dim):\n",
    "#         super(AttentionCombine, self).__init__()\n",
    "#         # Combine the attention context vector with the decoder hidden state\n",
    "#         self.attn_combine = nn.Linear(latent_dim + embed_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, attn_weights, encoder_outputs, hidden):\n",
    "#         \"\"\"\n",
    "#         Combine the context vector with the hidden state.\n",
    "        \n",
    "#         Args:\n",
    "#             attn_weights: Attention weights (batch_size, seq_len)\n",
    "#             encoder_outputs: All hidden states from the encoder (batch_size, seq_len, latent_dim)\n",
    "#             hidden: Previous hidden state of the decoder (batch_size, embed_dim)\n",
    "        \n",
    "#         Returns:\n",
    "#             combined_input: Combined input to pass to the LSTM (batch_size, embed_dim)\n",
    "#         \"\"\"\n",
    "#         # Compute the context vector by applying attention weights to the encoder outputs\n",
    "#         context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, latent_dim)\n",
    "#         context = context.squeeze(1)  # (batch_size, latent_dim)\n",
    "\n",
    "#         # Concatenate the context vector with the previous hidden state\n",
    "#         combined_input = torch.cat((context, hidden), dim=1)  # (batch_size, latent_dim + embed_dim)\n",
    "\n",
    "#         # Pass through the attention-combine linear layer\n",
    "#         combined_input = torch.tanh(self.attn_combine(combined_input))  # (batch_size, embed_dim)\n",
    "        \n",
    "#         return combined_input\n",
    "\n",
    "# class DecoderWithAttention(nn.Module):\n",
    "#     def __init__(self, output_dim, latent_dim, embed_dim, num_layers=3):\n",
    "#         super(DecoderWithAttention, self).__init__()\n",
    "        \n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_layers = num_layers\n",
    "        \n",
    "#         # LSTM layers for decoding\n",
    "#         self.lstm = nn.LSTM(embed_dim, embed_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "#         # Attention mechanism and attention-combine layer\n",
    "#         self.attn = Attention(latent_dim, embed_dim)\n",
    "#         self.attn_combine = AttentionCombine(latent_dim, embed_dim)\n",
    "\n",
    "#         # Fully connected layer to project the latent space to output embedding space\n",
    "#         self.fc_out = nn.Linear(embed_dim, output_dim)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "#     def forward(self, latent_vectors, encoder_outputs, hidden):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             latent_vectors: The latent vectors from the encoder (batch_size, seq_len, latent_dim)\n",
    "#             encoder_outputs: The encoder hidden states for attention (batch_size, seq_len, latent_dim)\n",
    "#             hidden: Previous hidden state of the LSTM (num_layers, batch_size, embed_dim)\n",
    "            \n",
    "#         Returns:\n",
    "#             output: The predicted output (batch_size, seq_len, output_dim)\n",
    "#             hidden: The updated hidden state (num_layers, batch_size, embed_dim)\n",
    "#         \"\"\"\n",
    "#         batch_size = latent_vectors.size(0)\n",
    "#         seq_len = latent_vectors.size(1)\n",
    "        \n",
    "#         # Initialize the output list\n",
    "#         outputs = torch.zeros(batch_size, seq_len, self.fc_out.out_features).to(latent_vectors.device)\n",
    "\n",
    "#         # Process each time step (sequentially) through the decoder\n",
    "#         for t in range(seq_len):\n",
    "#             # Get the attention weights from the previous hidden state and encoder outputs\n",
    "#             attn_weights = self.attn(hidden[-1], encoder_outputs)\n",
    "\n",
    "#             # Combine the attention context vector with the current latent vector\n",
    "#             lstm_input = self.attn_combine(attn_weights, encoder_outputs, latent_vectors[:, t])\n",
    "\n",
    "#             # LSTM forward pass\n",
    "#             lstm_out, hidden = self.lstm(lstm_input.unsqueeze(1), hidden)  # lstm_out: (batch_size, 1, embed_dim)\n",
    "#             lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "#             # Predict the output at the current time step\n",
    "#             outputs[:, t] = self.fc_out(lstm_out.squeeze(1))\n",
    "\n",
    "#         return outputs, hidden\n",
    "\n",
    "#     def init_hidden(self, batch_size):\n",
    "#         \"\"\" Initialize hidden and cell states for the LSTM layers \"\"\"\n",
    "#         h0 = torch.zeros(self.num_layers, batch_size, self.embed_dim).to(next(self.parameters()).device)\n",
    "#         c0 = torch.zeros(self.num_layers, batch_size, self.embed_dim).to(next(self.parameters()).device)\n",
    "#         return (h0, c0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
