{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Translation is a sequence-to-sequence task, we can use very basic encoder decoder architecture for this, using LSTMs.\n",
    "# But it fails for longer sentences, as we were not able to summarize longer sentences into context vectors.\n",
    "# Then came the concept of attention based mechanisms, where we send different context vectors for decoding to the decoder, according to different weights assigned to each hidden state, main aim is to learn the attention weights.\n",
    "# Especially for sentences > 30 words and so.\n",
    "# But it was very slow, as sequential learning is happening here, so takes much longer time for larger datasets.\n",
    "# Transfer learning (i.e. model is already trained on a very large dataset, then we just fine tune the model), but here we can't train these models on larger datasets, so transfer learning can't happen here.\n",
    "# Sequential learning is mainly because of RNNs LSTMs, as they take in sequential data only.\n",
    "\n",
    "# This problem in transformers was solved by\n",
    "# By a new paper called 'Attention is all you need'\n",
    "# In 2007 by Google Research and Google Brain\n",
    "\n",
    "# Now there is no LSTM / RNN units in this transformer architecture\n",
    "# This mechanism is called self-attention\n",
    "# We could train this architecture in parallel, which allowed the concept of transfer learning.\n",
    "\n",
    "# Instead of LSTM -> We use Self-Attention which allows parallel training -> Transfer Learning\n",
    "# A lot of hyperparameters are used in this\n",
    "# This paper, was not a incremental paper, it was completely new architecture used in this.\n",
    "\n",
    "# It gives us\n",
    "# Scalability\n",
    "# Transformers allow transfer learning\n",
    "# Multimodal (text, images, speech etc)\n",
    "# Flexible architecture (can be extended to multiple types of transformers)\n",
    "\n",
    "# Hugging face (library for Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention Mechanism in Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-lingual language models (XLMs)\n",
    "# monolingual data (unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
