{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¤ Une coalition (BBY) composée de plus d’une c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$Mais le groupe a aussi annoncé la suppression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$Toutefois, l’étude menée a également livré de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>€Dans les locaux d’Atem, à Solliès-Pont, les é...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>° 1.000.000 de dirhams hors taxes pour les mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>С’est son premier voyage à l’étranger depuis s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>С’est un établissement d'enseignement supérieu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>Сергей ГунеевAccéder à la base multimédiaVladi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>يا تونِسَ الأُنسِ يا خَضرا المَيادينِ », a écr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>チーズ, c’est le mot japonais pour “fromage”.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "0       ¤ Une coalition (BBY) composée de plus d’une c...\n",
       "1       $Mais le groupe a aussi annoncé la suppression...\n",
       "2       $Toutefois, l’étude menée a également livré de...\n",
       "3       €Dans les locaux d’Atem, à Solliès-Pont, les é...\n",
       "4       ° 1.000.000 de dirhams hors taxes pour les mar...\n",
       "...                                                   ...\n",
       "999995  С’est son premier voyage à l’étranger depuis s...\n",
       "999996  С’est un établissement d'enseignement supérieu...\n",
       "999997  Сергей ГунеевAccéder à la base multimédiaVladi...\n",
       "999998  يا تونِسَ الأُنسِ يا خَضرا المَيادينِ », a écr...\n",
       "999999       チーズ, c’est le mot japonais pour “fromage”.\\n\n",
       "\n",
       "[1000000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load French Monolingual Dataset\n",
    "sentences = []\n",
    "\n",
    "with open('./Dataset/Monolingual/fra_news_2023_1M/fra_news_2023_1M-sentences.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split('\\t', 1)\n",
    "        if len(parts) > 1:\n",
    "            sentence = parts[1]\n",
    "            sentences.append(sentence)\n",
    "\n",
    "french_df = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "french_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$10,000 Gold?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAN FRANCISCO – It has never been easy to have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lately, with gold prices up more than 300% ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just last December, fellow economists Martin F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wouldn’t you know it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901593</th>\n",
       "      <td>At the same time, Zuma’s revolutionary generat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901594</th>\n",
       "      <td>In a region that reveres the elderly, Zuma’s a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901595</th>\n",
       "      <td>Three in ten South Africans are younger than 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901596</th>\n",
       "      <td>Somehow Zuma must find a way to honor his own ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901597</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>901598 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "0                                           $10,000 Gold?\n",
       "1       SAN FRANCISCO – It has never been easy to have...\n",
       "2       Lately, with gold prices up more than 300% ove...\n",
       "3       Just last December, fellow economists Martin F...\n",
       "4                                   Wouldn’t you know it?\n",
       "...                                                   ...\n",
       "901593  At the same time, Zuma’s revolutionary generat...\n",
       "901594  In a region that reveres the elderly, Zuma’s a...\n",
       "901595  Three in ten South Africans are younger than 1...\n",
       "901596  Somehow Zuma must find a way to honor his own ...\n",
       "901597                                                   \n",
       "\n",
       "[901598 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English Monolingual Dataset\n",
    "with open('./Dataset/Monolingual/news-commentary-v18.txt', 'r', encoding='utf-8') as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "english_df = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "english_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 20\n",
    "french_df = french_df[:x]\n",
    "english_df = english_df[:x]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty sentences if any\n",
    "english_df = english_df[english_df['Sentence'].notna() & (english_df['Sentence'] != '')]\n",
    "french_df = french_df[french_df['Sentence'].notna() & (french_df['Sentence'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)  # Remove punctuation\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.loc[:, 'Sentence'] = english_df['Sentence'].apply(lambda x: preprocess_text(x))\n",
    "french_df.loc[:, 'Sentence'] = french_df['Sentence'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remerge_sent(sent):\n",
    "    # merges tokens which are not separated by white-space\n",
    "    # does this recursively until no further changes\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        i = 0\n",
    "        while i < sent.__len__() - 1:\n",
    "            tok = sent[i]\n",
    "            if not tok.whitespace_:\n",
    "                ntok = sent[i + 1]\n",
    "                # in-place operation.\n",
    "                with sent.retokenize() as retokenizer:\n",
    "                    retokenizer.merge(sent[i: i + 2])\n",
    "                changed = True\n",
    "            i += 1\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_spacy(sentence, nlp):\n",
    "    doc = nlp(sentence)\n",
    "    spacy_sentence = remerge_sent(doc)\n",
    "    return [token.text for token in spacy_sentence]\n",
    "\n",
    "# Tokenize\n",
    "english_df['Tokens'] = english_df['Sentence'].apply(lambda x: tokenize_with_spacy(x, nlp_en))\n",
    "french_df['Tokens'] = french_df['Sentence'].apply(lambda x: tokenize_with_spacy(x, nlp_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop sentences longer than 50 words\n",
    "english_df = english_df[english_df['Tokens'].apply(len) <= 50]\n",
    "french_df = french_df[french_df['Tokens'].apply(len) <= 50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate BERT Embeddings\n",
    "\n",
    "We can use pre trained transformer models for generating embeddings for the tokens, we can use different versions of BERT for generation embeddings for French, German etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT tokenizer and model\n",
    "# Load the fast version of the tokenizer\n",
    "\n",
    "tokenizer_en = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "bert_model_en = AutoModel.from_pretrained('bert-base-uncased')\n",
    "bert_model_en = bert_model_en.to(device)\n",
    "\n",
    "tokenizer_fr = AutoTokenizer.from_pretrained('camembert-base', use_fast=True)\n",
    "bert_model_fr = AutoModel.from_pretrained('camembert-base')\n",
    "bert_model_fr = bert_model_fr.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves BERT embeddings for a list of tokens using a fast tokenizer, enabling accurate aggregation of subword embeddings into their original token representations.\n",
    "def get_bert_embeddings(tokens, tokenizer, bert_model):\n",
    "    inputs = tokenizer(tokens, return_tensors='pt', is_split_into_words=True, padding=False, truncation=True)\n",
    "\n",
    "    # Get BERT embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Get the embeddings for each subword\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: (sequence_length, hidden_size)\n",
    "    # Get word_ids to align subword tokens with the original tokens\n",
    "    word_ids = inputs.word_ids()\n",
    "\n",
    "    # Aggregate subword embeddings back to their original tokens\n",
    "    aggregated_embeddings = []\n",
    "    current_token_embeddings = []\n",
    "\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        if len(current_token_embeddings) > 0 and word_id != word_ids[idx - 1]:\n",
    "            aggregated_embeddings.append(torch.mean(torch.stack(current_token_embeddings), dim=0))\n",
    "            current_token_embeddings = []\n",
    "        current_token_embeddings.append(token_embeddings[idx])\n",
    "    \n",
    "    if len(current_token_embeddings) > 0:\n",
    "        aggregated_embeddings.append(torch.mean(torch.stack(current_token_embeddings), dim=0))\n",
    "\n",
    "    return torch.stack(aggregated_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate BERT embeddings for dataFrame\n",
    "def generate_embeddings(df, tokenizer, bert_model):\n",
    "    embeddings_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        tokenized_sentence = row['Tokens']\n",
    "        embeddings = get_bert_embeddings(tokenized_sentence, tokenizer, bert_model)\n",
    "        embeddings_list.append(embeddings)\n",
    "    return embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_embeddings(df, embedding_list):\n",
    "    # Checking the length of longest sentence\n",
    "    max_len = max(embedding.shape[0] for embedding in embedding_list)\n",
    "    print(f\"The length of the longest sentence is: {max_len}\")\n",
    "\n",
    "    # Pad the embeddings to ensure uniformity across dataset as model will be trained in batches\n",
    "    padded_embeddings = pad_sequence(embedding_list, batch_first=True)\n",
    "    df['Embeddings'] = [padded_embeddings[i] for i in range(padded_embeddings.shape[0])]\n",
    "    print(padded_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_embeddings = generate_embeddings(english_df, tokenizer_en, bert_model_en)\n",
    "english_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest sentence is: 31\n",
      "torch.Size([20, 31, 768])\n"
     ]
    }
   ],
   "source": [
    "generate_padded_embeddings(english_df, english_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_embeddings = generate_embeddings(french_df, tokenizer_fr, bert_model_fr)\n",
    "french_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest sentence is: 35\n",
      "torch.Size([20, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "generate_padded_embeddings(french_df, french_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Auto Encoding (DAE)\n",
    "\n",
    "Noise is added to make sure that self auto encoding mechanism is not just returning the same sequence of words, and rather learns the structure of the sentence, otherwise for a random sequence of words also it will return the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_dropout(sentence, pwd=0.1, padding_embedding=None):\n",
    "    noisy_sentence = []\n",
    "    drop_count = 0\n",
    "\n",
    "    for word in sentence:\n",
    "        if random.random() > pwd:\n",
    "            noisy_sentence.append(word)\n",
    "        else:\n",
    "            drop_count += 1\n",
    "\n",
    "    if padding_embedding is None:\n",
    "        padding_embedding = torch.zeros_like(sentence[0]) \n",
    "\n",
    "    noisy_sentence.extend([padding_embedding] * drop_count)\n",
    "    noisy_sentence_tensor = torch.stack(noisy_sentence)\n",
    "\n",
    "    return noisy_sentence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentence_shuffling(sentence, k=3, alpha=0.5):\n",
    "    n = sentence.size(0)\n",
    "\n",
    "    # Generating random permutation vector q\n",
    "    q = torch.arange(n).float() + torch.rand(n) * alpha  # Slightly perturb the indices\n",
    "    _, permuted_indices = torch.sort(q)  # Sorting to get the permutation\n",
    "\n",
    "    # Apply the shuffle, respecting the condition |σ(i) - i| <= k\n",
    "    for i in range(n):\n",
    "        if abs(permuted_indices[i] - i) > k:\n",
    "            permuted_indices[i] = i \n",
    "    \n",
    "    # Shuffle sentence according to the permuted indices\n",
    "    shuffled_sentence = sentence[permuted_indices]\n",
    "    \n",
    "    return shuffled_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise(x, pwd=0.1, k=3, alpha=0.5):\n",
    "    if isinstance(x, list):\n",
    "        x = torch.tensor(x)\n",
    "\n",
    "    # Apply word dropout\n",
    "    x_noisy = apply_word_dropout(x, pwd)\n",
    "    \n",
    "    # Apply sentence shuffling\n",
    "    x_noisy = apply_sentence_shuffling(x_noisy, k, alpha)\n",
    "    \n",
    "    return x_noisy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Decoder With Attention\n",
    "\n",
    "This is mainly used for longer sentences. As the sentence gets longer, the context vector $(z)$ passed as the first input to the decoder gets diminished or forgotten, and the decoder loses its attention or focus on the main parts of the sequence, causing a drop in the BLEU score.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "The decoder generates the target sentence by iteratively producing each word $y_i$, based on:\n",
    "\n",
    "$$\n",
    "p(y_i \\mid y_1, \\dots, y_{i-1}, x) = g(y_{i-1}, s_i, c_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_i$ is the target word at time step $i$,\n",
    "- $s_i$ is the hidden state of the decoder LSTM at time step $i$,\n",
    "- $c_i$ is the context vector that summarizes the relevant information from the encoder.\n",
    "\n",
    "---\n",
    "\n",
    "#### Decoder Hidden State Update\n",
    "\n",
    "The hidden state of the decoder at each time step $i$ is computed by an RNN / LSTM update. The state is conditioned on the previous hidden state $(s_{i-1})$, the previously generated word $(y_{i-1})$, and the context vector $(c_i)$. The context vector contains information from the encoder and helps the decoder focus on the most relevant parts of the source sentence.\n",
    "\n",
    "$$\n",
    "s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $s_i$ is the decoder's hidden state at time step $i$,\n",
    "- $s_{i-1}$ is the hidden state from the previous time step,\n",
    "- $y_{i-1}$ is the previously generated target word,\n",
    "- $c_i$ is the context vector at time step $i$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Context Vector\n",
    "\n",
    "The context vector $c_i$ is a weighted sum of the encoder's hidden states (annotations). It reflects the parts of the source sentence that the decoder should focus on when generating the target word $y_i$. Each hidden state $h_j$ from the encoder is weighted by an attention weight $\\alpha_{ij}$, which indicates how much attention to pay to that part of the input sentence at time step $i$.\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $h_j$ is the encoder hidden state at position $j$,\n",
    "- $\\alpha_{ij}$ is the attention weight for position $j$ at decoding time step $i$,\n",
    "- $T_x$ is the length of the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "#### Attention Weights\n",
    "\n",
    "The attention weight $\\alpha_{ij}$ represents how much focus the decoder should place on the $j$-th hidden state of the encoder when generating the $i$-th target word. These attention weights are computed using a softmax function, which ensures that the weights sum to 1 across all positions in the input sequence. This mechanism allows the decoder to pay varying degrees of attention to different parts of the input sequence at each step.\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $e_{ij}$ is the alignment score for the $j$-th input word and the $i$-th target word,\n",
    "- The denominator sums over all alignment scores for input positions $k$, normalizing the attention weights.\n",
    "\n",
    "---\n",
    "\n",
    "#### Alignment Score\n",
    "\n",
    "The alignment score $e_{ij}$ is computed by a feedforward neural network, known as the **alignment model**. It measures how well the decoder's previous hidden state $s_{i-1}$ aligns with the encoder's hidden state $h_j$. This score helps decide which parts of the source sentence are most relevant when predicting the next target word.\n",
    "\n",
    "$$\n",
    "e_{ij} = a(s_{i-1}, h_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $s_{i-1}$ is the decoder hidden state at time step $i-1$,\n",
    "- $h_j$ is the encoder hidden state at position $j$,\n",
    "- $a$ is the alignment model, which can be a simple feedforward network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM Encoder\n",
    "\n",
    "Generates sequence of hidden (latent) states for a given sequence of input embeddings (src/tgt language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True, num_layers=num_layers)\n",
    "        \n",
    "        # Fully connected layers for final prediction\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()  # Activation function\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Apply dropout and batch normalization\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = self.batch_norm(lstm_out.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        fc_output = self.activation(self.fc1(lstm_out))  # (batch_size, seq_len, hidden_dim)\n",
    "        fc_output = self.activation(self.fc2(fc_output))  # (batch_size, seq_len, hidden_dim // 2)\n",
    "        latent_output = self.fc3(fc_output)  # (batch_size, seq_len, output_dim)\n",
    "        \n",
    "        return lstm_out, latent_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Decoder\n",
    "\n",
    "Generates sequence of words in a (src/tgt) language based on the previous hidden state of the decoder, the current word (hidden state from the encoder), and a context vector given by a weighted sum over the encoder output states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Attention mechanism to compute attention weights\n",
    "        self.attn = nn.Linear(hidden_dim + latent_dim, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Compute the attention weights and context vector.\n",
    "        \n",
    "        Args:\n",
    "            hidden: The hidden state of the decoder (batch_size, hidden_dim)\n",
    "            encoder_outputs: The outputs from the encoder (batch_size, seq_len, latent_dim)\n",
    "        \n",
    "        Returns:\n",
    "            attn_weights: The attention weights (batch_size, seq_len)\n",
    "            context: The context vector (batch_size, latent_dim)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        # Expand hidden state for concatenation with encoder outputs\n",
    "        hidden = hidden.unsqueeze(1).expand(batch_size, seq_len, -1)\n",
    "\n",
    "        # Concatenate hidden state and encoder outputs\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Compute attention weights using the dot product with v\n",
    "        energy = energy.transpose(1, 2)  # (batch_size, hidden_dim, seq_len)\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # Batched matrix multiplication (bmm)\n",
    "        attn_weights = torch.bmm(v, energy).squeeze(1)  # (batch_size, seq_len)\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # Normalize attention weights\n",
    "\n",
    "        # Compute the context vector as a weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (batch_size, latent_dim)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, num_layers=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer for decoding\n",
    "        # Takes input as the context vector of hidden_dim and output from the encoder of latent_dim\n",
    "        # Outputs a hidden state of hidden_dim\n",
    "        self.lstm = nn.LSTM(latent_dim + hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attn = Attention(hidden_dim, latent_dim)\n",
    "\n",
    "        # Fully connected layer to output the prediction\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, latent_vectors, encoder_outputs, hidden):\n",
    "        \"\"\"\n",
    "        Decoder forward pass with attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            latent_vectors: The latent vectors from the encoder (batch_size, seq_len, latent_dim), output given by bidirectional LSTM encoder after summarizing (abstracting) the input sentence, that is passing through the fc layers.\n",
    "            They will be used as input for decoder LSTM, for finding the target word.\n",
    "\n",
    "            encoder_outputs: The encoder outputs (batch_size, seq_len, latent_dim), output given by bidirectional LSTM encoder of dim 300.\n",
    "            They will by used for finding the attention weights and context vector.\n",
    "            \n",
    "            hidden: The hidden state from the previous time step (num_layers, batch_size, hidden_dim)\n",
    "            \n",
    "        Returns:\n",
    "            outputs: The generated output sequence (batch_size, seq_len, output_dim)\n",
    "            hidden: The updated hidden state (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = latent_vectors.size(0)\n",
    "        seq_len = latent_vectors.size(1)\n",
    "\n",
    "        # Initialize the output tensor\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.fc_out.out_features).to(latent_vectors.device)\n",
    "\n",
    "        # Iterate through the sequence to generate each word\n",
    "        for t in range(seq_len):\n",
    "            # Get attention weights and context vector\n",
    "            # hidden[-1] gives the previous hidden state\n",
    "            context = self.attn(hidden[-1], encoder_outputs)\n",
    "\n",
    "            # Concatenate the context vector with the latent vector (decoder input)\n",
    "            lstm_input = torch.cat((context, latent_vectors[:, t]), dim=1).unsqueeze(1)\n",
    "\n",
    "            # Pass through the LSTM\n",
    "            lstm_out, hidden = self.lstm(lstm_input, hidden)  # lstm_out: (batch_size, 1, hidden_dim)\n",
    "            lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "            # Predict the next word\n",
    "            outputs[:, t] = self.fc_out(lstm_out.squeeze(1))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" Initialize hidden state and cell state for LSTM \"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the DAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "NUM_LSTM_LAYERS = 3\n",
    "LEARNING_RATE = 0.0003\n",
    "\n",
    "# Encoder\n",
    "ENCODER_INPUT_DIM = 768 \n",
    "ENCODER_HIDDEN_DIM = 300\n",
    "ENCODER_OUTPUT_DIM = 100 \n",
    "\n",
    "# Decoder\n",
    "DECODER_OUTPUT_DIM = ENCODER_INPUT_DIM\n",
    "DECODER_LATENT_DIM = ENCODER_OUTPUT_DIM\n",
    "DECODER_HIIDEN_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x and y are same because of auto-encoding\n",
    "english_dataset = TensorDataset(\n",
    "    torch.stack(english_df['Embeddings'].tolist()),\n",
    "    torch.stack(english_df['Embeddings'].tolist()),\n",
    ")\n",
    "english_loader = DataLoader(\n",
    "    english_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "french_dataset = TensorDataset(\n",
    "    torch.stack(french_df['Embeddings'].tolist()),\n",
    "    torch.stack(french_df['Embeddings'].tolist()),\n",
    ")\n",
    "french_loader = DataLoader(\n",
    "    french_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder_model = Encoder(\n",
    "    ENCODER_INPUT_DIM,\n",
    "    ENCODER_HIDDEN_DIM, \n",
    "    ENCODER_OUTPUT_DIM,\n",
    "    NUM_LSTM_LAYERS,\n",
    ")\n",
    "\n",
    "decoder_model = Decoder(\n",
    "    DECODER_LATENT_DIM,\n",
    "    DECODER_HIIDEN_DIM,\n",
    "    DECODER_OUTPUT_DIM,\n",
    "    NUM_LSTM_LAYERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (lstm): LSTM(400, 300, num_layers=3, batch_first=True)\n",
       "  (attn): Attention(\n",
       "    (attn): Linear(in_features=400, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=300, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the models to train mode\n",
    "encoder_model.train()\n",
    "decoder_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer (Adam)\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder_model.parameters()) + list(decoder_model.parameters()), \n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (Cosine Loss)\n",
    "def cosine_similarity_loss(predictions, targets, pad_idx=0): \n",
    "    # Flatten the predictions and targets\n",
    "    predictions = predictions.view(-1, predictions.size(-1))  # (batch_size * seq_len, embedding_dim)\n",
    "    targets = targets.view(-1, targets.size(-1))  # (batch_size * seq_len, embedding_dim)\n",
    "\n",
    "    # Create a mask to exclude padded positions\n",
    "    mask = (targets.sum(dim=-1) != pad_idx).float()  # (batch_size * seq_len,)\n",
    "\n",
    "    # Compute the cosine similarity between the predictions and targets\n",
    "    cos_sim = F.cosine_similarity(predictions, targets, dim=-1)  # (batch_size * seq_len,)\n",
    "    loss = (1 - cos_sim) * mask  # Apply mask to exclude padding positions\n",
    "    loss = loss.sum() / mask.sum()  # Average the loss over non-padding tokens\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training the model\n",
    "def train_dae(encoder_model, decoder_model, optimizer, data_loader, clip, pad_idx=0):\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        encoder_model.train()\n",
    "        decoder_model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in data_loader:\n",
    "            inputs = batch[0].to(device)  # BERT embeddings\n",
    "            targets = batch[1].to(device)  # Same because of autoencoding\n",
    "            \n",
    "            # Add noise to inputs and stack them into a tensor\n",
    "            noisy_inputs = torch.stack([apply_noise(sentence) for sentence in inputs]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through encoder\n",
    "            encoder_outputs, latent_vectors = encoder_model(noisy_inputs)\n",
    "            \n",
    "            # Initialize decoder hidden state for each batch\n",
    "            hidden, _ = decoder_model.init_hidden(inputs.size(0))  \n",
    "\n",
    "            # Decoder forward pass with attention\n",
    "            outputs, hidden = decoder_model(latent_vectors, encoder_outputs, hidden)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = cosine_similarity_loss(outputs, targets, pad_idx)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(encoder_model.parameters(), clip)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder_model.parameters(), clip)\n",
    "\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Print the loss for each epoch\n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {epoch_loss / len(data_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (620x900 and 400x300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train DAE for English Dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dae(\n\u001b[1;32m      3\u001b[0m     encoder_model,\n\u001b[1;32m      4\u001b[0m     decoder_model,\n\u001b[1;32m      5\u001b[0m     optimizer,\n\u001b[1;32m      6\u001b[0m     english_loader,\n\u001b[1;32m      7\u001b[0m     clip\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m )\n",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m, in \u001b[0;36mtrain_dae\u001b[0;34m(encoder_model, decoder_model, optimizer, data_loader, clip, pad_idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m hidden, _ \u001b[39m=\u001b[39m decoder_model\u001b[39m.\u001b[39minit_hidden(inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))  \n\u001b[1;32m     24\u001b[0m \u001b[39m# Decoder forward pass with attention\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs, hidden \u001b[39m=\u001b[39m decoder_model(latent_vectors, encoder_outputs, hidden)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m cosine_similarity_loss(outputs, targets, pad_idx)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 48\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, latent_vectors, encoder_outputs, hidden)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m# Iterate through the sequence to generate each word\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len):\n\u001b[1;32m     46\u001b[0m     \u001b[39m# Get attention weights and context vector\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[39m# hidden[-1] gives the previous hidden state\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(hidden[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], encoder_outputs)\n\u001b[1;32m     50\u001b[0m     \u001b[39m# Concatenate the context vector with the latent vector (decoder input)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     lstm_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((context, latent_vectors[:, t]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 33\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m concatenated \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden_expanded, encoder_outputs), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# (batch_size, seq_len, hidden_dim + latent_dim)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Compute the attention energy (batch_size, seq_len, hidden_dim)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m energy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(concatenated))  \u001b[39m# (batch_size, seq_len, hidden_dim)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# Transpose energy to shape (batch_size, hidden_dim, seq_len)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m energy \u001b[39m=\u001b[39m energy\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# (batch_size, hidden_dim, seq_len)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (620x900 and 400x300)"
     ]
    }
   ],
   "source": [
    "# Train DAE for English Dataset\n",
    "train_dae(\n",
    "    encoder_model,\n",
    "    decoder_model,\n",
    "    optimizer,\n",
    "    english_loader,\n",
    "    clip=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
