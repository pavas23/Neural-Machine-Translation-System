{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L’or à 10.000 dollars l’once ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAN FRANCISCO – Il n’a jamais été facile d’avo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Et aujourd’hui, alors que le cours de l’or a a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En décembre dernier, mes collègues économistes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mais devinez ce qui s’est passé ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466575</th>\n",
       "      <td>Il fait partie de la génération fière d’avoir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466576</th>\n",
       "      <td>Mais il semblerait cependant que cette générat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466577</th>\n",
       "      <td>Dans une région qui révère les personnes âgées...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466578</th>\n",
       "      <td>3 Sud-africains sur 10 ont moins de 15 ans, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466579</th>\n",
       "      <td>D’une manière ou d’une autre, Zuma doit trouve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466580 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "0                          L’or à 10.000 dollars l’once ?\n",
       "1       SAN FRANCISCO – Il n’a jamais été facile d’avo...\n",
       "2       Et aujourd’hui, alors que le cours de l’or a a...\n",
       "3       En décembre dernier, mes collègues économistes...\n",
       "4                       Mais devinez ce qui s’est passé ?\n",
       "...                                                   ...\n",
       "466575  Il fait partie de la génération fière d’avoir ...\n",
       "466576  Mais il semblerait cependant que cette générat...\n",
       "466577  Dans une région qui révère les personnes âgées...\n",
       "466578  3 Sud-africains sur 10 ont moins de 15 ans, ce...\n",
       "466579  D’une manière ou d’une autre, Zuma doit trouve...\n",
       "\n",
       "[466580 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load French Monolingual Dataset\n",
    "sentences = []\n",
    "\n",
    "with open('/Users/atharvadashora/Downloads/news-commentary-v17.fr', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()  # Remove any leading/trailing whitespace\n",
    "        if line:  # Only add non-empty lines\n",
    "            sentences.append(line)\n",
    "\n",
    "# Create a DataFrame with each sentence\n",
    "french_df = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "french_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$10,000 Gold?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAN FRANCISCO – It has never been easy to have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lately, with gold prices up more than 300% ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just last December, fellow economists Martin F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wouldn’t you know it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901593</th>\n",
       "      <td>At the same time, Zuma’s revolutionary generat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901594</th>\n",
       "      <td>In a region that reveres the elderly, Zuma’s a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901595</th>\n",
       "      <td>Three in ten South Africans are younger than 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901596</th>\n",
       "      <td>Somehow Zuma must find a way to honor his own ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901597</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>901598 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "0                                           $10,000 Gold?\n",
       "1       SAN FRANCISCO – It has never been easy to have...\n",
       "2       Lately, with gold prices up more than 300% ove...\n",
       "3       Just last December, fellow economists Martin F...\n",
       "4                                   Wouldn’t you know it?\n",
       "...                                                   ...\n",
       "901593  At the same time, Zuma’s revolutionary generat...\n",
       "901594  In a region that reveres the elderly, Zuma’s a...\n",
       "901595  Three in ten South Africans are younger than 1...\n",
       "901596  Somehow Zuma must find a way to honor his own ...\n",
       "901597                                                   \n",
       "\n",
       "[901598 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English Monolingual Dataset\n",
    "with open('/Users/atharvadashora/Downloads/news-commentary-v18.en', 'r', encoding='utf-8') as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "english_df = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "english_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "french_df = french_df[:x]\n",
    "english_df = english_df[:x]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LANGS = 2\n",
    "MAX_SEQ_LEN = 20\n",
    "\n",
    "lang_idx_mapping = {\n",
    "    'english': 0,\n",
    "    'french': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty sentences if any\n",
    "english_df = english_df[english_df['Sentence'].notna() & (english_df['Sentence'] != '')]\n",
    "french_df = french_df[french_df['Sentence'].notna() & (french_df['Sentence'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$10,000 Gold?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAN FRANCISCO – It has never been easy to have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lately, with gold prices up more than 300% ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just last December, fellow economists Martin F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wouldn’t you know it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Since their articles appeared, the price of go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gold prices even hit a record-high $1,300 rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Last December, many gold bugs were arguing tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Now, emboldened by continuing appreciation, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>One successful gold investor recently explaine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence\n",
       "0                                      $10,000 Gold?\n",
       "1  SAN FRANCISCO – It has never been easy to have...\n",
       "2  Lately, with gold prices up more than 300% ove...\n",
       "3  Just last December, fellow economists Martin F...\n",
       "4                              Wouldn’t you know it?\n",
       "5  Since their articles appeared, the price of go...\n",
       "6  Gold prices even hit a record-high $1,300 rece...\n",
       "7  Last December, many gold bugs were arguing tha...\n",
       "8  Now, emboldened by continuing appreciation, so...\n",
       "9  One successful gold investor recently explaine..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)  # Remove punctuation\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.loc[:, 'Sentence'] = english_df['Sentence'].apply(lambda x: preprocess_text(x))\n",
    "french_df.loc[:, 'Sentence'] = french_df['Sentence'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remerge_sent(sent):\n",
    "    # merges tokens which are not separated by white-space\n",
    "    # does this recursively until no further changes\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        i = 0\n",
    "        while i < sent.__len__() - 1:\n",
    "            tok = sent[i]\n",
    "            if not tok.whitespace_:\n",
    "                ntok = sent[i + 1]\n",
    "                # in-place operation.\n",
    "                with sent.retokenize() as retokenizer:\n",
    "                    retokenizer.merge(sent[i: i + 2])\n",
    "                changed = True\n",
    "            i += 1\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.7.0/fr_core_news_md-3.7.0-py3-none-any.whl (45.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from fr-core-news-md==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/mach/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-md==3.7.0) (2.1.3)\n",
      "Installing collected packages: fr-core-news-md\n",
      "Successfully installed fr-core-news-md-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_spacy(sentence, nlp):\n",
    "    doc = nlp(sentence)\n",
    "    spacy_sentence = remerge_sent(doc)\n",
    "    return [token.text for token in spacy_sentence]\n",
    "\n",
    "# Tokenize\n",
    "english_df['Tokens'] = english_df['Sentence'].apply(lambda x: tokenize_with_spacy(x, nlp_en))\n",
    "french_df['Tokens'] = french_df['Sentence'].apply(lambda x: tokenize_with_spacy(x, nlp_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop sentences longer than 50 words\n",
    "english_df = english_df[english_df['Tokens'].apply(len) <= MAX_SEQ_LEN]\n",
    "french_df = french_df[french_df['Tokens'].apply(len) <= MAX_SEQ_LEN]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate BERT Embeddings\n",
    "\n",
    "We can use pre trained transformer models for generating embeddings for the tokens, we can use different versions of BERT for generation embeddings for French, German etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT tokenizer and model\n",
    "# Load the fast version of the tokenizer\n",
    "\n",
    "tokenizer_en = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "bert_model_en = AutoModel.from_pretrained('bert-base-uncased')\n",
    "bert_model_en = bert_model_en.to(device)\n",
    "vocab_embeddings_en = bert_model_en.embeddings.word_embeddings.weight  # shape: (vocab_size, 768)\n",
    "\n",
    "tokenizer_fr = AutoTokenizer.from_pretrained('camembert-base', use_fast=True)\n",
    "bert_model_fr = AutoModel.from_pretrained('camembert-base')\n",
    "bert_model_fr = bert_model_fr.to(device)\n",
    "vocab_embeddings_fr = bert_model_fr.embeddings.word_embeddings.weight  # shape: (vocab_size, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves BERT embeddings for a list of tokens using a fast tokenizer, enabling accurate aggregation of subword embeddings into their original token representations.\n",
    "def get_bert_embeddings(tokens, tokenizer, bert_model):\n",
    "    inputs = tokenizer(tokens, return_tensors='pt', is_split_into_words=True, padding=False, truncation=True)\n",
    "\n",
    "    # Get BERT embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Get the embeddings for each subword\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: (sequence_length, hidden_size)\n",
    "    # Get word_ids to align subword tokens with the original tokens\n",
    "    word_ids = inputs.word_ids()\n",
    "\n",
    "    # Aggregate subword embeddings back to their original tokens\n",
    "    aggregated_embeddings = []\n",
    "    current_token_embeddings = []\n",
    "\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        if len(current_token_embeddings) > 0 and word_id != word_ids[idx - 1]:\n",
    "            aggregated_embeddings.append(torch.mean(torch.stack(current_token_embeddings), dim=0))\n",
    "            current_token_embeddings = []\n",
    "        current_token_embeddings.append(token_embeddings[idx])\n",
    "    \n",
    "    if len(current_token_embeddings) > 0:\n",
    "        aggregated_embeddings.append(torch.mean(torch.stack(current_token_embeddings), dim=0))\n",
    "\n",
    "    return torch.stack(aggregated_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate BERT embeddings for dataFrame\n",
    "def generate_embeddings(df, tokenizer, bert_model):\n",
    "    embeddings_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        tokenized_sentence = row['Tokens']\n",
    "        embeddings = get_bert_embeddings(tokenized_sentence, tokenizer, bert_model)\n",
    "        embeddings_list.append(embeddings)\n",
    "    return embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_embeddings(df, embedding_list, max_len=MAX_SEQ_LEN):\n",
    "    # Pad the sequences using pad_sequence. It will pad them to the length of the longest sentence.\n",
    "    padded_embeddings = pad_sequence(embedding_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Truncate the sequences if they are longer than max_len\n",
    "    if padded_embeddings.size(1) > max_len:\n",
    "        padded_embeddings = padded_embeddings[:, :max_len, :]\n",
    "    # If sequences are shorter than max_len, pad them manually\n",
    "    elif padded_embeddings.size(1) < max_len:\n",
    "        padding_size = max_len - padded_embeddings.size(1)\n",
    "        padded_embeddings = F.pad(padded_embeddings, (0, 0, 0, padding_size), value=0)\n",
    "    \n",
    "    # Assign the padded embeddings to the dataframe\n",
    "    df['Embeddings'] = [padded_embeddings[i] for i in range(padded_embeddings.shape[0])]\n",
    "    print(f\"Padded Embeddings Shape: {padded_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_embeddings = generate_embeddings(english_df, tokenizer_en, bert_model_en)\n",
    "english_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Embeddings Shape: torch.Size([8, 20, 768])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generate_padded_embeddings(english_df, english_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embeddings = generate_embeddings(french_df, tokenizer_fr, bert_model_fr)\n",
    "french_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_padded_embeddings(french_df, french_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Auto Encoding (DAE)\n",
    "\n",
    "Noise is added to make sure that self auto encoding mechanism is not just returning the same sequence of words, and rather learns the structure of the sentence, otherwise for a random sequence of words also it will return the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_dropout(sentence, pwd=0.1, padding_embedding=None):\n",
    "    noisy_sentence = []\n",
    "    drop_count = 0\n",
    "\n",
    "    for word in sentence:\n",
    "        if random.random() > pwd:\n",
    "            noisy_sentence.append(word)\n",
    "        else:\n",
    "            drop_count += 1\n",
    "\n",
    "    if padding_embedding is None:\n",
    "        padding_embedding = torch.zeros_like(sentence[0]) \n",
    "\n",
    "    noisy_sentence.extend([padding_embedding] * drop_count)\n",
    "    noisy_sentence_tensor = torch.stack(noisy_sentence)\n",
    "\n",
    "    return noisy_sentence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentence_shuffling(sentence, k=3, alpha=0.5):\n",
    "    n = sentence.size(0)\n",
    "\n",
    "    # Generating random permutation vector q\n",
    "    q = torch.arange(n).float() + torch.rand(n) * alpha  # Slightly perturb the indices\n",
    "    _, permuted_indices = torch.sort(q)  # Sorting to get the permutation\n",
    "\n",
    "    # Apply the shuffle, respecting the condition |σ(i) - i| <= k\n",
    "    for i in range(n):\n",
    "        if abs(permuted_indices[i] - i) > k:\n",
    "            permuted_indices[i] = i \n",
    "    \n",
    "    # Shuffle sentence according to the permuted indices\n",
    "    shuffled_sentence = sentence[permuted_indices]\n",
    "    \n",
    "    return shuffled_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise(x, pwd=0.1, k=3, alpha=0.5):\n",
    "    if isinstance(x, list):\n",
    "        x = torch.tensor(x)\n",
    "\n",
    "    # Apply word dropout\n",
    "    x_noisy = apply_word_dropout(x, pwd)\n",
    "    \n",
    "    # Apply sentence shuffling\n",
    "    x_noisy = apply_sentence_shuffling(x_noisy, k, alpha)\n",
    "    \n",
    "    return x_noisy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Decoder With Attention\n",
    "\n",
    "This is mainly used for longer sentences. As the sentence gets longer, the context vector $(z)$ passed as the first input to the decoder gets diminished or forgotten, and the decoder loses its attention or focus on the main parts of the sequence, causing a drop in the BLEU score.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "The decoder generates the target sentence by iteratively producing each word $y_i$, based on:\n",
    "\n",
    "$$\n",
    "p(y_i \\mid y_1, \\dots, y_{i-1}, x) = g(y_{i-1}, s_i, c_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_i$ is the target word at time step $i$,\n",
    "- $s_i$ is the hidden state of the decoder LSTM at time step $i$,\n",
    "- $c_i$ is the context vector that summarizes the relevant information from the encoder.\n",
    "\n",
    "---\n",
    "\n",
    "#### Decoder Hidden State Update\n",
    "\n",
    "The hidden state of the decoder at each time step $i$ is computed by an RNN / LSTM update. The state is conditioned on the previous hidden state $(s_{i-1})$, the previously generated word $(y_{i-1})$, and the context vector $(c_i)$. The context vector contains information from the encoder and helps the decoder focus on the most relevant parts of the source sentence.\n",
    "\n",
    "$$\n",
    "s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $s_i$ is the decoder's hidden state at time step $i$,\n",
    "- $s_{i-1}$ is the hidden state from the previous time step,\n",
    "- $y_{i-1}$ is the previously generated target word,\n",
    "- $c_i$ is the context vector at time step $i$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Context Vector\n",
    "\n",
    "The context vector $c_i$ is a weighted sum of the encoder's hidden states (annotations). It reflects the parts of the source sentence that the decoder should focus on when generating the target word $y_i$. Each hidden state $h_j$ from the encoder is weighted by an attention weight $\\alpha_{ij}$, which indicates how much attention to pay to that part of the input sentence at time step $i$.\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $h_j$ is the encoder hidden state at position $j$,\n",
    "- $\\alpha_{ij}$ is the attention weight for position $j$ at decoding time step $i$,\n",
    "- $T_x$ is the length of the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "#### Attention Weights\n",
    "\n",
    "The attention weight $\\alpha_{ij}$ represents how much focus the decoder should place on the $j$-th hidden state of the encoder when generating the $i$-th target word. These attention weights are computed using a softmax function, which ensures that the weights sum to 1 across all positions in the input sequence. This mechanism allows the decoder to pay varying degrees of attention to different parts of the input sequence at each step.\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $e_{ij}$ is the alignment score for the $j$-th input word and the $i$-th target word,\n",
    "- The denominator sums over all alignment scores for input positions $k$, normalizing the attention weights.\n",
    "\n",
    "---\n",
    "\n",
    "#### Alignment Score\n",
    "\n",
    "The alignment score $e_{ij}$ is computed by a feedforward neural network, known as the **alignment model**. It measures how well the decoder's previous hidden state $s_{i-1}$ aligns with the encoder's hidden state $h_j$. This score helps decide which parts of the source sentence are most relevant when predicting the next target word.\n",
    "\n",
    "$$\n",
    "e_{ij} = a(s_{i-1}, h_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $s_{i-1}$ is the decoder hidden state at time step $i-1$,\n",
    "- $h_j$ is the encoder hidden state at position $j$,\n",
    "- $a$ is the alignment model, which can be a simple feedforward network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM Encoder\n",
    "\n",
    "Generates sequence of hidden (latent) states for a given sequence of input embeddings (src/tgt language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, lang_embed_dim=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim + lang_embed_dim, hidden_dim, bidirectional=True, batch_first=True, num_layers=num_layers)\n",
    "        \n",
    "        # Fully connected layers for final prediction\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()  # Activation function\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n",
    "\n",
    "        # Language embedding layer\n",
    "        self.lang_embedding = nn.Embedding(NUM_LANGS, lang_embed_dim)  \n",
    "\n",
    "    def forward(self, inputs, lang_idx):\n",
    "        # Get language embedding\n",
    "        lang_emb = self.lang_embedding(lang_idx).unsqueeze(1).expand(-1, inputs.size(1), -1)\n",
    "\n",
    "        # Concatenate the language embedding with the input embeddings\n",
    "        inputs_with_lang = torch.cat((inputs, lang_emb), dim=-1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(inputs_with_lang)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Apply dropout and batch normalization\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = self.batch_norm(lstm_out.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        fc1_output = self.activation(self.fc1(lstm_out))  # (batch_size, seq_len, hidden_dim)\n",
    "        fc2_output = self.activation(self.fc2(fc1_output))  # (batch_size, seq_len, hidden_dim // 2)\n",
    "        latent_output = self.fc3(fc2_output)  # (batch_size, seq_len, output_dim)\n",
    "        \n",
    "        # fc1_output has shape 300*1\n",
    "        return fc1_output, latent_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Decoder\n",
    "\n",
    "Generates sequence of words in a (src/tgt) language based on the previous hidden state of the decoder, the current word (hidden state from the encoder), and a context vector given by a weighted sum over the encoder output states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Attention mechanism to compute attention weights\n",
    "        # Concatenate hidden state and encoder outputs, and project to hidden_dim\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)  # Input dim = 2 * hidden_dim, Output = hidden_dim\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))      # Attention vector for scoring\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Compute the attention weights and context vector.\n",
    "        \n",
    "        Args:\n",
    "            hidden: The hidden state of the decoder (batch_size, hidden_dim)\n",
    "            encoder_outputs: The outputs from the encoder (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        Returns:\n",
    "            context: The context vector (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden_dim = encoder_outputs.size(2)\n",
    "\n",
    "        # Expand hidden state to match the sequence length\n",
    "        hidden_expanded = hidden.unsqueeze(1).expand(batch_size, seq_len, hidden_dim)  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Concatenate hidden state and encoder outputs along the last dimension\n",
    "        concatenated = torch.cat((hidden_expanded, encoder_outputs), dim=2)  # (batch_size, seq_len, 2 * hidden_dim)\n",
    "\n",
    "        # Pass through the attention network\n",
    "        energy = torch.tanh(self.attn(concatenated))  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Transpose energy to prepare for batch matrix multiplication\n",
    "        energy = energy.transpose(1, 2)  # (batch_size, hidden_dim, seq_len)\n",
    "\n",
    "        # Repeat v to match the batch size (for each batch, we use the same attention vector)\n",
    "        v_expanded = self.v.repeat(batch_size, 1).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # Compute attention scores using batch matrix multiplication\n",
    "        attn_weights = torch.bmm(v_expanded, energy).squeeze(1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Normalize attention weights using softmax\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Compute the context vector as the weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, num_layers=3, lang_embed_dim=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer for decoding\n",
    "        # Takes input as the context vector of hidden_dim and output from the encoder of latent_dim\n",
    "        # Outputs a hidden state of hidden_dim\n",
    "        self.lstm = nn.LSTM(latent_dim + hidden_dim + lang_embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attn = Attention(hidden_dim)\n",
    "\n",
    "        # Fully connected layer to output the prediction\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Language embedding layer\n",
    "        self.lang_embedding = nn.Embedding(NUM_LANGS, lang_embed_dim)\n",
    "\n",
    "    def forward(self, latent_vectors, encoder_outputs, hidden, lang_idx):\n",
    "        \"\"\"\n",
    "        Decoder forward pass with attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            latent_vectors: The latent vectors from the encoder (batch_size, seq_len, latent_dim), output given by bidirectional LSTM encoder after summarizing (abstracting) the input sentence, that is passing through the fc layers.\n",
    "            They will be used as input for decoder LSTM, for finding the target word.\n",
    "\n",
    "            encoder_outputs: The encoder outputs (batch_size, seq_len, latent_dim), output given by bidirectional LSTM encoder of dim 300.\n",
    "            They will by used for finding the attention weights and context vector.\n",
    "            \n",
    "            hidden: The hidden state from the previous time step (num_layers, batch_size, hidden_dim)\n",
    "            \n",
    "        Returns:\n",
    "            outputs: The generated output sequence (batch_size, seq_len, output_dim)\n",
    "            hidden: The updated hidden state (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = latent_vectors.size(0)\n",
    "        seq_len = latent_vectors.size(1)\n",
    "\n",
    "        # Get language embedding\n",
    "        lang_emb = self.lang_embedding(lang_idx).unsqueeze(1).expand(-1, latent_vectors.size(1), -1)\n",
    "\n",
    "        # Initialize the output tensor\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.fc_out.out_features).to(latent_vectors.device)\n",
    "\n",
    "        # Iterate through the sequence to generate each word\n",
    "        for t in range(seq_len):\n",
    "            # Get attention weights and context vector\n",
    "            # hidden[0][-1] gives the previous hidden state, need to take hidden[0] as hidden is a tuple (h,c)\n",
    "            context = self.attn(hidden[0][-1], encoder_outputs)\n",
    "\n",
    "            # Concatenate the context vector, latent vector, and language embedding\n",
    "            lstm_input = torch.cat((context, latent_vectors[:, t], lang_emb[:, t]), dim=1).unsqueeze(1)\n",
    "\n",
    "            # Pass through the LSTM\n",
    "            lstm_out, hidden = self.lstm(lstm_input, hidden)  # lstm_out: (batch_size, 1, hidden_dim)\n",
    "            lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "            # Predict the next word\n",
    "            outputs[:, t] = self.fc_out(lstm_out.squeeze(1))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" Initialize hidden state and cell state for LSTM \"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the DAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "NUM_LSTM_LAYERS = 3\n",
    "LEARNING_RATE = 0.0003\n",
    "LANG_DIM = 32\n",
    "\n",
    "# Encoder\n",
    "ENCODER_INPUT_DIM = 768 \n",
    "ENCODER_HIDDEN_DIM = 300\n",
    "ENCODER_OUTPUT_DIM = 100 \n",
    "\n",
    "# Decoder\n",
    "DECODER_OUTPUT_DIM = ENCODER_INPUT_DIM\n",
    "DECODER_LATENT_DIM = ENCODER_OUTPUT_DIM\n",
    "DECODER_HIIDEN_DIM = 300"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language index has been added for each sentence, as we are using ```same encoder and decoder``` instance for source and target languages, i.e ```weights are being shared```, so encoder takes in the language index and creates a language embedding which is appended to the input word embedding, also the decoder takes the output generated by encoder, context vector and also the langauge vector to produce output sequence in the given language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x and y are same because of auto-encoding\n",
    "english_dataset = TensorDataset(\n",
    "    torch.stack(english_df['Embeddings'].tolist()),\n",
    "    torch.stack(english_df['Embeddings'].tolist()),\n",
    "    torch.tensor([lang_idx_mapping['english']] * len(english_df))\n",
    ")\n",
    "english_loader = DataLoader(\n",
    "    english_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "french_dataset = TensorDataset(\n",
    "    torch.stack(french_df['Embeddings'].tolist()),\n",
    "    torch.stack(french_df['Embeddings'].tolist()),\n",
    "    torch.tensor([lang_idx_mapping['french']] * len(french_df))\n",
    ")\n",
    "french_loader = DataLoader(\n",
    "    french_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "combined_dataset = torch.utils.data.ConcatDataset([english_dataset, french_dataset])\n",
    "combined_loader = DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder_model = Encoder(\n",
    "    ENCODER_INPUT_DIM,\n",
    "    ENCODER_HIDDEN_DIM, \n",
    "    ENCODER_OUTPUT_DIM,\n",
    "    NUM_LSTM_LAYERS,\n",
    ")\n",
    "\n",
    "decoder_model = Decoder(\n",
    "    DECODER_LATENT_DIM,\n",
    "    DECODER_HIIDEN_DIM,\n",
    "    DECODER_OUTPUT_DIM,\n",
    "    NUM_LSTM_LAYERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the models to train mode\n",
    "encoder_model.train()\n",
    "decoder_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer (Adam)\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder_model.parameters()) + list(decoder_model.parameters()), \n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (Cosine Loss)\n",
    "def cosine_similarity_loss(predictions, targets, pad_idx=0): \n",
    "    # Flatten the predictions and targets\n",
    "    predictions = predictions.view(-1, predictions.size(-1))  # (batch_size * seq_len, embedding_dim)\n",
    "    targets = targets.view(-1, targets.size(-1))  # (batch_size * seq_len, embedding_dim)\n",
    "\n",
    "    # Create a mask to exclude padded positions\n",
    "    mask = (targets.sum(dim=-1) != pad_idx).float()  # (batch_size * seq_len,)\n",
    "\n",
    "    # Compute the cosine similarity between the predictions and targets\n",
    "    cos_sim = F.cosine_similarity(predictions, targets, dim=-1)  # (batch_size * seq_len,)\n",
    "    loss = (1 - cos_sim) * mask  # Apply mask to exclude padding positions\n",
    "    loss = loss.sum() / mask.sum()  # Average the loss over non-padding tokens\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))  \n",
    "\n",
    "    norm = plt.Normalize(vmin=min(losses), vmax=max(losses))\n",
    "    colors = plt.cm.cividis(norm(losses)) \n",
    "\n",
    "    for i in range(len(epochs) - 1):\n",
    "        alpha_value = max(0.3, 0.8 - i * 0.02) \n",
    "        ax.plot(epochs[i:i+2], losses[i:i+2], color=colors[i], alpha=alpha_value, linewidth=2)\n",
    "\n",
    "    ax.fill_between(epochs, losses, color='lightblue', alpha=0.3)\n",
    "\n",
    "    ax.set_title('Training Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Epochs', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap='cividis', norm=norm)\n",
    "    sm.set_array([]) \n",
    "    fig.colorbar(sm, ax=ax, label='Loss Intensity')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training the model\n",
    "def train_dae(encoder_model, decoder_model, optimizer, data_loader, clip, pad_idx=0):\n",
    "    all_losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        encoder_model.train()\n",
    "        decoder_model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in data_loader:\n",
    "            inputs = batch[0].to(device)  # BERT embeddings\n",
    "            targets = batch[1].to(device)  # Same because of autoencoding\n",
    "            lang_idx = batch[2].to(device)  # Get language index for each sentence\n",
    "            \n",
    "            # Add noise to inputs and stack them into a tensor\n",
    "            noisy_inputs = torch.stack([apply_noise(sentence) for sentence in inputs]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through encoder\n",
    "            encoder_outputs, latent_vectors = encoder_model(noisy_inputs, lang_idx)\n",
    "            \n",
    "            # Initialize decoder hidden state for each batch\n",
    "            hidden = decoder_model.init_hidden(inputs.size(0))  \n",
    "            \n",
    "            # Decoder forward pass with attention\n",
    "            outputs, hidden = decoder_model(latent_vectors, encoder_outputs, hidden, lang_idx)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = cosine_similarity_loss(outputs, targets, pad_idx)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(encoder_model.parameters(), clip)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder_model.parameters(), clip)\n",
    "\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "        all_losses.append(avg_epoch_loss)\n",
    "\n",
    "        # Print the loss for each epoch\n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {avg_epoch_loss:.4f}')\n",
    "    \n",
    "    plot_loss(all_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DAE for English Dataset\n",
    "train_dae(\n",
    "    encoder_model,\n",
    "    decoder_model,\n",
    "    optimizer,\n",
    "    combined_loader,\n",
    "    clip=1.0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Embeddings generated by Decoder to Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_token_from_vocab(embedding, vocab_embeddings):\n",
    "    # Find the closest token from the vocabulary for a given embedding.\n",
    "    similarities = F.cosine_similarity(embedding, vocab_embeddings, dim=1)\n",
    "    closest_token_idx = torch.argmax(similarities)\n",
    "    return closest_token_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_text(output_embeddings, tokenizer, vocab_embeddings):\n",
    "\n",
    "    batch_size, seq_len, embedding_dim = output_embeddings.size()\n",
    "    tokens = []\n",
    "    \n",
    "    # Iterate over each sequence in the batch\n",
    "    for i in range(batch_size):\n",
    "        sequence_tokens = []\n",
    "        \n",
    "        # Iterate over each embedding in the sequence\n",
    "        for j in range(seq_len):\n",
    "            embedding = output_embeddings[i, j]  # Shape: (embedding_dim,)\n",
    "            token_idx = get_closest_token_from_vocab(embedding, vocab_embeddings)\n",
    "            token = tokenizer.convert_ids_to_tokens(token_idx)\n",
    "            sequence_tokens.append(token)\n",
    "        \n",
    "        sentence = tokenizer.convert_tokens_to_string(sequence_tokens)\n",
    "        tokens.append(sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_using_dae(df, tokenizer, vocab_embeddings, lang_idx):\n",
    "    inputs = torch.stack([sentence for sentence in df['Embeddings']]).to(device)\n",
    "    lang_idx_tensor = torch.tensor([lang_idx] * len(df)).to(device)\n",
    "\n",
    "    encoder_outputs, latent_vectors = encoder_model(inputs, lang_idx_tensor)\n",
    "\n",
    "    # Initialize decoder hidden state for each batch\n",
    "    hidden = decoder_model.init_hidden(len(df['Embeddings']))  \n",
    "\n",
    "    # Decoder forward pass with attention\n",
    "    outputs, hidden = decoder_model(latent_vectors, encoder_outputs, hidden, lang_idx_tensor)\n",
    "\n",
    "    generated_text = embeddings_to_text(outputs, tokenizer, vocab_embeddings)\n",
    "\n",
    "    def clean_output(text):\n",
    "        return ' '.join(\n",
    "            [token for token in text.split() if token not in [\n",
    "                '[SEP]', '[SEP].', '[MASK]', '[CLS]'\n",
    "            ]]\n",
    "        )\n",
    "\n",
    "    # Clean the generated text\n",
    "    cleaned_text = [clean_output(sentence) for sentence in generated_text]\n",
    "    for sentence in cleaned_text:\n",
    "        print(\"Generated Sentence:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text for english\n",
    "generate_text_using_dae(english_df, tokenizer_en, vocab_embeddings_en, lang_idx_mapping['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text for french\n",
    "generate_text_using_dae(french_df, tokenizer_fr, vocab_embeddings_fr, lang_idx_mapping['french'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.activation(self.fc1(x)))\n",
    "        x = self.dropout(self.activation(self.fc2(x)))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid output for binary classification\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Encoder(ENCODER_INPUT_DIM, ENCODER_HIDDEN_DIM, ENCODER_OUTPUT_DIM, NUM_LSTM_LAYERS)\n",
    "decoder_model = Decoder(DECODER_LATENT_DIM, DECODER_HIIDEN_DIM, DECODER_OUTPUT_DIM, NUM_LSTM_LAYERS)\n",
    "discriminator_model = Discriminator(ENCODER_OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "enc_dec_optimizer = optim.Adam(list(encoder_model.parameters()) + list(decoder_model.parameters()), lr=LEARNING_RATE)\n",
    "disc_optimizer = optim.Adam(discriminator_model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross_domain(encoder_model, decoder_model, discriminator_model, enc_dec_optimizer, disc_optimizer, \n",
    "                       source_loader, target_loader, clip, lambda_cd=1.0, lambda_adv=0.5, pad_idx=0):\n",
    "    all_losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        encoder_model.train()\n",
    "        decoder_model.train()\n",
    "        discriminator_model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for source_batch, target_batch in zip(source_loader, target_loader):\n",
    "            # ---------------------------\n",
    "            # Step 1: Cross-Domain Translation Loss\n",
    "            # ---------------------------\n",
    "            enc_dec_optimizer.zero_grad()\n",
    "\n",
    "            # Source Domain\n",
    "            source_inputs = source_batch[0].to(device)\n",
    "            source_lang_idx = source_batch[1].to(device)\n",
    "\n",
    "            # Target Domain\n",
    "            target_inputs = target_batch[0].to(device)\n",
    "            target_lang_idx = target_batch[1].to(device)\n",
    "\n",
    "            # Forward pass through encoder and decoder for source -> target -> source\n",
    "            noisy_source_inputs = torch.stack([apply_noise(sentence) for sentence in source_inputs]).to(device)\n",
    "            source_enc_outputs, source_latents = encoder_model(noisy_source_inputs, source_lang_idx)\n",
    "            \n",
    "            # Translate from source to target\n",
    "            translated_to_target, _ = decoder_model(source_latents, source_enc_outputs, hidden=None, lang_idx=target_lang_idx)\n",
    "\n",
    "            # Re-encode translated target\n",
    "            target_enc_outputs, target_latents = encoder_model(translated_to_target, target_lang_idx)\n",
    "\n",
    "            # Back-translate to source domain\n",
    "            translated_back_to_source, _ = decoder_model(target_latents, target_enc_outputs, hidden=None, lang_idx=source_lang_idx)\n",
    "\n",
    "            # Compute cross-domain loss (reconstruction in original domain)\n",
    "            loss_cd = F.cross_entropy(translated_back_to_source, source_inputs)\n",
    "\n",
    "            # ---------------------------\n",
    "            # Step 2: Adversarial Loss\n",
    "            # ---------------------------\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            # Discriminator predicts source or target\n",
    "            source_latents = encoder_model(source_inputs, source_lang_idx)[1]\n",
    "            target_latents = encoder_model(target_inputs, target_lang_idx)[1]\n",
    "\n",
    "            disc_loss_src = F.binary_cross_entropy(discriminator_model(source_latents), torch.ones(source_latents.size(0), 1).to(device))\n",
    "            disc_loss_tgt = F.binary_cross_entropy(discriminator_model(target_latents), torch.zeros(target_latents.size(0), 1).to(device))\n",
    "            disc_loss = disc_loss_src + disc_loss_tgt\n",
    "\n",
    "            # Update discriminator\n",
    "            disc_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            # Generator (encoder) adversarial loss to fool discriminator\n",
    "            source_latents = encoder_model(source_inputs, source_lang_idx)[1]\n",
    "            adv_loss_gen = F.binary_cross_entropy(discriminator_model(source_latents), torch.zeros(source_latents.size(0), 1).to(device))\n",
    "\n",
    "            # ---------------------------\n",
    "            # Total Loss and Update Encoder-Decoder\n",
    "            # ---------------------------\n",
    "            total_loss = lambda_cd * loss_cd + lambda_adv * adv_loss_gen\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Clip gradients and optimize encoder-decoder\n",
    "            torch.nn.utils.clip_grad_norm_(encoder_model.parameters(), clip)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder_model.parameters(), clip)\n",
    "            enc_dec_optimizer.step()\n",
    "\n",
    "            # Accumulate total loss\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(source_loader)\n",
    "        all_losses.append(avg_epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Cross-Domain Loss: {loss_cd.item():.4f}, Adv Loss: {disc_loss.item():.4f}, Total Loss: {avg_epoch_loss:.4f}')\n",
    "    \n",
    "    plot_loss(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cross_domain(\n",
    "    encoder_model,\n",
    "    decoder_model,\n",
    "    discriminator_model,\n",
    "    enc_dec_optimizer,\n",
    "    disc_optimizer,\n",
    "    english_loader,\n",
    "    french_loader,\n",
    "    clip=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cross_domain_sentences(encoder, decoder, tokenizer, vocab_embeddings, input_sentences, source_lang_idx, target_lang_idx):\n",
    "    \"\"\"\n",
    "    Generate cross-domain sentences by encoding sentences from source to target and back.\n",
    "\n",
    "    Args:\n",
    "        encoder (nn.Module): Encoder model.\n",
    "        decoder (nn.Module): Decoder model.\n",
    "        tokenizer: Tokenizer for converting tokens to text.\n",
    "        vocab_embeddings: Vocabulary embeddings for decoding.\n",
    "        input_sentences (list of str): Sentences in the source language.\n",
    "        source_lang_idx (int): Language index for the source language.\n",
    "        target_lang_idx (int): Language index for the target language.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Cross-domain generated sentences in the source language.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Convert input sentences to embeddings or token IDs\n",
    "    input_embeddings = [tokenizer.encode(sentence) for sentence in input_sentences]\n",
    "    input_embeddings = torch.stack(input_embeddings).to(device)\n",
    "    source_lang_tensor = torch.tensor([source_lang_idx] * len(input_sentences)).to(device)\n",
    "\n",
    "    # Encode input sentences in the source language\n",
    "    _, latent_vectors = encoder(input_embeddings, source_lang_tensor)\n",
    "    \n",
    "    # Decode to target language\n",
    "    target_lang_tensor = torch.tensor([target_lang_idx] * len(input_sentences)).to(device)\n",
    "    encoder_outputs, _ = encoder(input_embeddings, source_lang_tensor)  # Assuming same model for cross domain\n",
    "    translated_to_target, hidden = decoder(latent_vectors, encoder_outputs, hidden=None, lang_idx=target_lang_tensor)\n",
    "\n",
    "    # Re-encode the target sentence embeddings\n",
    "    _, target_latents = encoder(translated_to_target, target_lang_tensor)\n",
    "\n",
    "    # Decode back to the source language\n",
    "    translated_back_to_source, _ = decoder(target_latents, encoder_outputs, hidden=None, lang_idx=source_lang_tensor)\n",
    "\n",
    "    # Convert back to text using vocabulary embeddings\n",
    "    generated_sentences = embeddings_to_text(translated_back_to_source, tokenizer, vocab_embeddings)\n",
    "\n",
    "    return generated_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_text(output_embeddings, tokenizer, vocab_embeddings):\n",
    "    \"\"\"\n",
    "    Convert output embeddings to readable text by finding closest tokens from the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        output_embeddings (torch.Tensor): Output embeddings from the decoder.\n",
    "        tokenizer: Tokenizer with vocab to convert token indices to words.\n",
    "        vocab_embeddings (torch.Tensor): Embeddings for the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of sentences as strings.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embedding_dim = output_embeddings.size()\n",
    "    tokens = []\n",
    "\n",
    "    # Iterate over each sequence in the batch\n",
    "    for i in range(batch_size):\n",
    "        sequence_tokens = []\n",
    "\n",
    "        # Iterate over each embedding in the sequence\n",
    "        for j in range(seq_len):\n",
    "            embedding = output_embeddings[i, j]  # Shape: (embedding_dim,)\n",
    "            token_idx = get_closest_token_from_vocab(embedding, vocab_embeddings)\n",
    "            token = tokenizer.convert_ids_to_tokens(token_idx)\n",
    "            sequence_tokens.append(token)\n",
    "\n",
    "        # Convert tokens to a sentence\n",
    "        sentence = tokenizer.convert_tokens_to_string(sequence_tokens)\n",
    "        tokens.append(sentence)\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_token_from_vocab(embedding, vocab_embeddings):\n",
    "    similarities = F.cosine_similarity(embedding.unsqueeze(0), vocab_embeddings, dim=1)\n",
    "    closest_token_idx = torch.argmax(similarities)\n",
    "    return closest_token_idx.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
